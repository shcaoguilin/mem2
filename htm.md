[原文](https://numenta.org/resources/HTM_CorticalLearningAlgorithms.pdf)

[toc]

## 1 多级临时记忆 概要
- 多级临时记忆 是 一个 机器学习技术, 它 的目标是 捕捉 新大脑皮层 的  结构和算法 属性.
- 新大脑皮层 是 智能 的 所在地, 在哺乳动物大脑中. 高 层 视觉,听觉,触觉,移动,语言,计划 都 是 由 新大脑皮层 执行的. 给定 这样 多 套 认知 功能, 你 或许 期待 新大脑皮层 实现 对等的 多套 神经 算法.  不是这样的情况. 新大脑皮层 显示 显著的 统一的 神经电路 模式 . 生物 证据 表明 新大脑皮层 实现了 一个 通用 算法 集  以 执行 多个 不同 智能 功能.
- 多级临时记忆 提供 一个 理论框架 来 理解 新大脑皮层 和 它的许多能力. 到目前为止 我们 已经 实现了 这个 理论 框架 的 一个 小 子集.  随着时间推移, 越来越多的 理论 将 被 实现. 今天 我们 相信 我们 已经 实现了 一个 足够的 子集, 关于 新大脑皮层 做了 什么, 以 具有 商用 和 科研 价值.
- 编程 多级临时记忆 不像 编程 传统 计算机. 今天的 计算机, 程序员 创建 特定的 程序 去 解决 特定 问题. 鲜明反差, 训练 多级临时记忆 通过 喂 给 感官数据流  . 多级临时记忆 的 能力 极大的 由 数据 决定.
- 多级临时记忆 可以 被 看作 一类 神经 网络. 按照 定义,  试图 去 建模 新大脑皮层 的 架构细节 的 系统 是 神经网络. 然而, 术语"神经网络" 不是 非常 有用 ,因为 它被 用到 各种 系统. 多级临时记忆 模型 的 神经元, 在一个hierarchy中, 被 组织 以 列、层、区. 按细节, 多级临时记忆 是 神经网络 的 一个 新形式.
- 如 名字 所 暗示 , 多级临时记忆 基本上 是 一个 基于 记忆 的 系统. 多级临时记忆 网络 用 许多 时变 数据 训练, 依赖于 存储的 模式 和 序列 的 大 集合. 数据 存储 和 访问 的方式 逻辑地 不同于 标准 模型 被 今天的 程序员 所用的.  经典 计算机 内存 是 一个 平的 组织 并且 没有 固有的 时间 概念. 一个 程序员 能 实现 任意 种类的 数据 组织 和 结构 在 平 计算机 内存 上. 他们 有 控制权 通过 如何、哪 信息 被 存储. 相反, 多级临时记忆 是 更 受约束的.  多级临时记忆 有 一个 hierarchy 组织 , 且 基于 固有的 时间. 信息 总是 存储 以 分布式的 样子.  多级临时记忆 的 用户 指定 hierarchy 尺寸, 系统 以 什么(数据) 训练, 但是 多级临时记忆 控制 信息  在哪、怎么 存储.
- 虽然 多级临时记忆 网络 显著的 不同于 经典 计算, (但) 我们 能 用 通用 目的 计算机 (去) 建模 他们 , 只要 我们 合并 关键 功能: 分层、时间分布表示、空间分布表示(稍后详叙). 我们 相信 假以时日, 专用 硬件 将 会 被 创建 去 生成 想要的 多级临时记忆 网络.
- 在 这个 文档 中, 我们 经常 表示 多级临时记忆 属性和原则 , 用 画的 例子 从 人类 视觉、触觉、听觉、语言和行为.  这些 例子 是 有用的, 因为 他们 是 直觉的 和 容易 抓住的. 但是, 它 是 重要的，心里 明白 多级临时记忆 能力 是 通用的。 他们 恰好 容易 换成 非人类 感官 输入 流，比如 雷达、红外线、 或  纯 信息 输入 流 像 金融 市场 数据、天气 数据、网页流量模式、文本。 多级临时记忆 是 学习并预测 机器 , 能 应用 到 多 种 问题。

### - 多级临时记忆 原理

- 在 这 段，我们 覆盖 多级临时记忆 核心 原理 的 一部分: 为什么 hierarchy 组织 是 重要的 ，如何 让 多级临时记忆 区 结构 化， 为什么 数据 被 存储为 稀疏 分布 表示，为什么 基于时间 信息 是 至少(要有)的.

#### - 分层
- 多级临时记忆 网络 包含 以 层次 组织 的 区.  在 多级临时记忆 中,  区 是 主 单位、预测, 在 下一 段　　 (区) 将 被 详细 讨论 . 典型地，每个 多级临时记忆 区 表示 在 hierarchy 中 的 一个 级别 . 当 你 向 上 hierarchy 走, 总 会 收敛,  在 一个 子 区 中 的 多个 元素 收敛 到 一个 父区 中 的 一个 元素. 然而，由于 反馈 连接，信息 也 发散 当你 向 下 hierarchy 走。（一个 区 和 一个 级别 几乎 是 同义词。 当 叙述 一个 区 的 内部 功能 时，我们 用 词 区；然而 当 特定地 提及 在 某个 hierarchy 内的 某个 区 的 某个 角色时，   我们 用 词 级别   ）
- 组合 多个 多级临时记忆 网络 是 可以的。  这种 结构 有 意义， 如果 你 有 数据 从 多个 源 或 传感器. 例如， 一个 网络 是 处理 听觉 信息 ，另一个 网络 是 处理 视觉 信息。 每个 单独 网络 是 收敛的, 而且 分离 的 分支 只 收敛  到 顶.
- hierarchy 组织 的 好处 是 效率. 它 显著地 降低了 训练 时间 和 内存 用量，因为 在 hierarchy 中  的 每个 级别 学到的 模式 被 重用， 当 在 更高 级 以 新奇的 方式 组合.  作为 例子， 让 我们 考虑 视觉.  在 hierarchy 的 最低 级， 你的 大脑 存储 关于    比如 可视域 边、角         微小 段 的 信息 。 一个 边 是  世界上 许多 物体 的 基础 组建 。   在 中级  的 这些  低 级  模式 被  重组合  进  像 曲线、质地 的 更 复杂 组件  。  一条 弧 可以 是 耳朵 的 边， 操纵 轮 的 顶端 或者 咖啡 杯 的 边框.   这些 中级  模式 进一步  组合 到 表示 高 级  目标 特征， 比如 头、车、房子。 为了 学习 新 高 级 目标 ， 你 不 需要 重学 它的 组件。
- 另一个 例子 ，考虑 当 你 学 一个 新 字， 你 不必 重学 字母、音节、音素。
- 在 一个 hierarchy 中 共用  表示  也 导致 期待 的 行为 的 泛化.  当 你 看 一个 新 动物， 如果 你 看到 一个 嘴巴、牙齿， 你会 预测 这个动物 用 它的 牙齿 吃，它 可能 咬 你。  hierarchy 启用 了 一个 新 目标 在 世界 上 去 继承 它的 子组件 的 已知 属性 。
- 在 一个 多级临时记忆 hierarchy 的  一个 单 级 能 学 到 多少？或者 换个 方式， 在 hierarchy 中 多少 级 是 必要的？ 在 分配 多少 内存 给 每 级 和 需要 多少 级 之间 需要 平衡。  进一步地， 多级临时记忆 自动 学习 最 可能的 表示 在 每 级     给定  输入 统计  和 资源 分配 量。  如果 你 分配 更多 内存 给 一 级 ，  此 级  将 形成  更大 更复杂的  表示 ， 它 依次 意味着  更少的 必要 hierarchy 级 。  如果 你 分配 更少 内存， 一 级 将 形成 更小 更简单 的 表示 ， 它 依次  意味着 更多的 必要 hierarchy 级 。
- 到 这 点，我们 正 叙述 困难 问题， 比如 视觉 推理 （推理 是 类似的 和 模式 识别）。  但是 许多 有价值的 问题 是 更简单 比 视觉， 一个 但 多级临时记忆 区 可能 满足.  例如， 我们 应用 一个 多级临时记忆 去  预测 ， 一个 人 浏览 一个 网站 ，接下来 会 点击 哪里。  这个 问题 涉及 喂 多级临时记忆 网络 以 网页 点击 数据。  在 这个 问题 中， 少有 或 没有 spatial hierarchy， 解 几乎 (只) 需要 发现 temporal 统计， 比如 靠 识别 典型 用户 模式 来 预测 该用户 将 点击 哪里。 在 多级临时记忆 (中) 这个 temporal 学习 算法 对 这类 问题 很 理想。
- 总结下， hierarchies 降低 训练 时间，降低 内存 用量，导致 一种 形式 的  泛化。 但是，用 一个 单 多级临时记忆 区  (可以) 解 许多 简单 预测 问题 。

#### - 区

- 记号 区，在 一个 hierarchy 中 用 导线 连接的，来自 生物学。新大脑皮层 是 一个 巨大 的  组织 簿片，大约 2毫米 厚。 生物学家 分 新大脑皮层 成 不同 区域 或者 区 ， 主要 基于 该区 怎么 连到 其他 区。有些 区 直接 接收 输入 从 感官 ，    另一些 区 接收  输入,只 在 它(该输入) 通过了 若干 其他 区 后。 区到区 的 连接性 定义了 hierarchy.
- 全部 新大脑皮层 区 看着 类似，在 他们的 细节上。他们 变化 ，以 尺寸 ,以 他们 在 hierarchy 中的 哪， 除此之外 他们 是 类似的。 如果 你 取 一块 薄片,穿透 一个 新大脑皮层 区 的 2毫米 厚度，  你 会 看到 6 层， 5层 细胞,1层 非细胞(有 一些 例外,但 这是 一般 规则(规律))。 在 一个 新大脑皮层 的 每 层  有 许多 内部连接 细胞,以 列 排列。  分层临时 区域 也 是 片装 组装，片 是 高度 内连 的 细胞， 细胞 排列 在 列 中。 新大脑皮层 中的 层3  是 神经元 主 前馈 层 之一。  一个 多级临时记忆 区 粗略 等于 ，新大脑皮层 一个 区 中 的 层3 中的 神经元们。
- 图1.3 : 一个 多级临时记忆 区 的 一个 段. 多级临时记忆 的 区们 由 许多 细胞 组成。  这些 细胞 在 列 中 以 二维 形式 组织. 这个 图 显示 一个 多级临时记忆 区 的 一个 小 段， 每 列 4个 细胞。 在这个区中，每 列 连接 到 输入 子集，每 细胞 连接 到 其他 细胞(连接没有显示). 注意 这个 多级临时记忆 区 ， 包含 它的 圆柱形(列) 结构 ， 等同于 在 一个 新大脑皮层 区 中 的 一 层 神经元.
- 虽然 一个 多级临时记忆 区 是 仅仅 等同于 新大脑皮层 中的 一份 区，它(该区) 能做 推理 和 预测 ,在 复杂 数据流 上,因此 (区 是) 有用的 在 许多 问题 (上).    

#### - 稀疏分布表示

- 虽然 新大脑皮层 (中的)  神经元 是 高度 内连的 ，抑制 神经元 保证了 仅仅 一 小 百分比 神经元 是 激活的 ,在 (任)一个 时刻. 因此，大脑 中 的 信息 总是 被 表示, 用 一个 巨大 人口(数目) 神经元 中的 一 小 百分比 的 激活 神经元。 这种 编码 叫做 稀疏分布表示。  稀疏 意思是 仅仅 一 小 百分比 的 神经元 是 激活的 , 在 (任)一个 时刻.  分布 意思是 , 为了 表示 某个 事物, 需要 激活 多个 神经元. 一个 单 激活 神经元 表达 某个 意思, 但 它 必须 被 解释,在  表达 整个含义(该事物) 的 全体 神经元   的 上下文 (中).
- 多级临时记忆 区们 也 用 稀疏分布表示。  事实上， 在 一个 多级临时记忆 区 内 的 记忆 原理(结构)(机制) 依赖于 使用 稀疏分布表示，否则 (区) 不工作。 给 一个 多级临时记忆 区 的 输入 总是 一个 分布表示，但 它 可能 不是 稀疏的，所以 一个 分布临时记忆 区 要做 的 第一件 事 是 转化 它的 输入 到(为)(成) 一个 稀疏分布表示.
- 例如，一个 区 可以 接收 2万 bit 的 输入。  输入 bit 中 1 、0 的 百分比 随 时间 显著 变化.  一时刻 可能 有 5千个  bit 1, 另一 时刻 可能 有 9千个  bit 1。  该 分册临时记忆 可以 转化 这个 输入 到 一个 1万 bit 的 内部 表示 , 它(该内部表示) 一次  有 2% 即 200个 bit 是 激活 的 ， 不论 这个 输入 中 有 多少 bit 1. 因为 这个 多级临时记忆 区 的 输入 随 时间 变化， 内部 表示 也(随时间) 变化，但是 1万 bit 中 总是 只有 200 bit 是 激活的。
- 看起来,这个 过程 产生了 信息 的 巨大 丢失,因为 可能的 输入 模式 数量 远多于 在 该 区 中  可能的 表示 数量。  然而， 这 两个 数 都是 巨 大的。一个 区 看到的 该 实际 输入们  是 全体 可能 输入 中的 很小的 小部分。 稍后 我们 会 叙述 一个 区 如何 从 它的 输入 创建 一个 稀疏表示. 信息 的 理论上 的 损失 不会 有 实用 效应.
- 图1.4 一个 多级临时记忆 区 正显示 稀疏 分布的 细胞 激活
- 稀疏分布表示 有 若干 吸引人的 属性,而且 被 集成 到 多级临时记忆 的 操作 . 后面 还会 再 讲到.

#### - 时间角色

- 时间 扮演 一个 决定性的 校色, 在 学习、推理、预测 中.
- 让 我们 从 推理 开始。  不 使用 时间，我们 几乎 不能 从 我们的 触觉、听觉 推理 (任何东西). 比如 ， 如果 你 被 蒙上 眼睛, (然后) 有人 方 一个 苹果 在 你 手上， 在 你 操纵 它(苹果) 仅仅 1秒 后，你 (就能) 知道 它 是 什么。当 你 在 苹果 上 移动 你的 手指 ，虽然 触觉 信息 不断 变化，目标 自身 -- 苹果，如同 你的 高层 感知 "苹果", 保持不变。但是，如果 一个 苹果 放在 你 张开的 手掌 中, 并且 不 允许 你 移动 你的 手 或 手指 ， 你 会 很难 分清 它 是 一个 苹果 还是 一个 柠檬。
- 对于 听觉 也是 一样 成立。 一个 静态的 声音 表达 极少的 意思。一个 字 ,比如 苹果,或 某人 咬 一个 苹果 的 咯吱 声音，只能 被 识别 出来， 从 音谱的 一打或几百个 急流、随时间的 连续的 变化 。
- 相比之下，视觉，是个 混合 情况。 不像 触觉、听觉，人类 能 识别 图像,当 他们(图像) 瞬间  闪亮(出现) 在 他们(人类) 面前,  (出现的) 太快 以至于 没给 眼睛 机会 去 移动(译者:没给眼睛移动的机会,因此视觉能力没有利用时变输入). 因此，视觉 推理 不 总是 需要 时变 输入。  但是，在 正常 视觉 期间，我们 不断 移动 我们的 眼睛、头、身体，并且 世界上 的 目标(物体) 也 在 我们的 周围 移动。 我们的 推理 能力,基于 快速 画面 曝光, 是 一个 特 例, 由 视觉 统计 属性 和 多年 训练  制造的. 视觉、听觉、触觉 的 通用 情形 是 基于  需要 时变 输入 的 推理(译者认为 作者这里有点故意往自己的东西上套的嫌疑 忽略了解释不了的事实).
- 刚刚 覆盖了 推理的 通用 情况， 特殊 情况: 静态 图像 的 视觉 推理 ， 现在 让 我们 看下 学习。  为了 去 学习，在 训练期间 , 所有 多级临时记忆 系统 必须 面对 时变 输入 .  即使 在 视觉 上, 静态 推理 有时 是 可能的，我们 必须 看 物体们 的 变化 图像 , 去 学习 一个 物体 看起 像 什么。 例如，想象 一只 狗 正 朝你 跑 来。 在 时间上 的 每一个 瞬间 , 这只狗 导致了 一个 激活 模式,在 你 眼睛 的 视网膜 上。你 知觉到 这些 模式 是 一个 相同的 狗 的 不同 view，但是 数学地(数学上看) 这些模式 是 完全地 不相似。**大脑 学到 这些 不同的 模式 意味着 相同的 东西 ，靠 按 序列地 观察 他们 (的方式). 时间 是 监督者， 教 你 哪些 空间 模式 放在 一起。**
- 注意 对于 感官 输入 仅仅 去 改变 是 不够的 。 你 学 会 辨别 狗们 ,  靠 看 很多  不同 品种 的 狗 实例 ， 而 不是 只有 一 单个 狗 的 一 单个 view。多级临时记忆 算法 的 工作 是 去 学 临时 序列 , 从 一个 输入 数据 流， 比如 构建 一个 模型,哪些 模式 跟在 哪些 其他 模式 后面。  这个 工作 是 困难的,因为 它 可能 不知道 序列 的 开始 、 结束，在 相同时刻 可能 有 覆盖 序列 出现，   学习 必须 连续地 发生， 并且 学习 必须 在 噪音 存在 (的情形下) 发生 .
- 学习、识别 序列 是 形成 预测 的 基础。 一旦 一个 多级临时记忆 学到 什么 模式们 很 可能 跟随 其他 模式们, 给定 当前 输入、直接 过去 输入们,它(多级临时记忆) 能 预测 很可能的 接下来的 模式们.  稍后 会 覆盖 到 预测 的 更多 细节 。
- 我们 现在 将 转向 多级临时记忆 的 四个 基本 功能：学习、推理、预测、行为。 每一个 多级临时记忆 区 执行 前 三个 功能：学习、推理、预测。 然而，行为 是 不同的。从 生物学,我们 知道 新大脑皮层 的 大多数 区们 有 一个 角色,创建 行为,  但是 对于 许多 有趣的 应用,我们 不相信 它(行为) 是 必需品。 因此 在 我们 当前 多级临时记忆 实现 中,我们 没 包含 行为. 这里 我们 提及 它(行为),是 出于 完整性.

### - 学习

- 一个 多级临时记忆 区 了解 它的 世界,靠 在 感官 数据 中 找 模式、then 序列。(该)区 不知道 输入 代表 什么；它 工作,以 一个 纯粹的 统计 领域。 **它(该区) 寻找 输入 bits 的  组合, 经常 出现 在 一起 的,(该组合) 我们 称之为  空间 模式们。 然后，它(该区) 寻找 这些 空间 模式们 怎么 出现,按 时刻 序列, (该时刻序列) 我们 称之为 临时 模式们 或 序列们。**
- 如果 到 (该)区 的 输入 表示 在 一个 建筑物 (中的) 环境 传感器们，(该)区 可能 会 发现  温度、湿度 的  特定 组合们,在 (该)建筑物 北边 经常 出现,而且 在 (该)建筑物 南边 出现 不同的 组合。然后,随着 每天 流逝, 它(该区) 可能 学到 这些 组合们 的 序列们 .
- 如果 到 一个 区 的输入 表示 在 一个 店 内 的  跟  购买们 相关的 信息， 多级临时记忆 的 (该)区 可能 发现,在周末 特定 类型们 的 物品们 被 购买，或者 当 天气 冷 了,在晚上,特定 价格 范围们 被 偏爱。然后 它(该区) 学到 不同的 个人 跟随 相似的 序列 模式们,在 他们 的 购买 中。
- 一个 单 多级临时记忆 区 有 受限的 学习 能力。 一个 区 自动地 调整 它(该区) 学 什么,基于 它(该区) 有 多少 内存,它(该区) 收到的 输入 的 复杂度。被 一个 区 学到的 该 空间 模式们 会 必然地 变的 更简单,如果 分配 给 一个 区 的 内存 被 降低. 或者 学到的 空间 模式们 可以 变得 更 复杂,如果 分配的 内存 被 增加了。 在 一个 区, 如果 学到的 空间 模式们 是 简单的,那么 区们 的 (一个) hierarchy 可能 需要 区 理解 复杂 图像们。 我们 看到 这个 特性,在 人类 视觉 系统 中,新大脑皮层 区 接收 输入 从 视网膜,(区) 学到 可视空间的 小 部分们 的 空间 模式们。只有 在 若干个 hierarchy级 后,做 空间 模式们 组合 并 表示 可视化 空间 的  大多数 或 全部。
- 像 一个 生物学 系统 一样， 在 一个 多级临时记忆 区 中 的  学习 算法 有 在线 学习 能力，例如 他们(区们) 连续地 从 每个 新输入 学习 。  这 不是 必要的,对于 学习 阶段 和 推理 阶段 分离，虽然 在 额外的 学习 后,推理 改善了. 当 在 输入 中的 模式们 改变，多级临时记忆 区 也 会 逐渐地 改变。
- 在 初始 训练 后，一个 多级临时记忆 能 连续 学习 ，或，在 训练 阶段 后 学习 可以 被 禁用。 另一个 选项 是 只 在 该hierarchy的 最低 级,关掉学习,但 在 更高 级 继续 学习。一旦  一个 分层例是记忆 已经 学到 它的 世界 的 基本 统计 结构，大多数 新 学习 出现 在 该hierarchy 的 更 上层 。 如果 一个 多级临时记忆 被 暴露 到 新 模式,之前 没 见过的 低级 结构，对 多级临时记忆,它 将 花 更久 去 学到 这些 新 模式。在 人类 中，我们 看到了 这特点。在 一个 你 已经 知道的  语言 中,学习 新 词们 是 相对 容易的。 然而，从 一个 不熟悉 的 发音 的 外语 中，如果 你 试图 学 新 词们，你 会 发现 这 非常 困难,因为 你 不 已经 知道 低级 发音。
- 简单地 发现 模式 是 一个 潜在 价值 能力。 在 市场 波动、灾难、气象、理解、制造商 投资收益,或 复杂系统 失效,如 电网， 理解 高 级  模式 是 有 价值的 。即便 如此，多数情况下, 学习 空间 、 临时 模式 是 推理、预测 的 先驱。

### - 推理

- 在 一个 多级临时记忆 已经 学会 这个 世界的 模式，它 能 在 新奇的 输入 上 执行 推理。 当 一个 多级临时记忆 收到 输入 ，它 将 匹配 先前 学到的 空间、时间 模式们。 成功地 匹配 新 输入 到 先前 存储的 序列们 是 推理、模式匹配 的 精要。
- 考虑 下 你 如何 识别 一个 曲子 的。听到 曲子 中的  第一个 音符,告诉你 很少。第二个 音符 显著地 缩小了 可能性,但 它 或许 仍然 不够。 通常 听 三、四 或 更多个 音符,你 你 会 识别 出 (该)曲子。在 一个 多级临时记忆 的 区 中 的  推理,是 类似的。 它 不断地 看 (一个) 输入 流,并匹配 他们 到 先前 学到的 序列们。一个 多级临时记忆 区 能 发现 匹配,从 序列们 的 开始,但 通常 它(开始处) 是 更 流动的(不定的)，类似 与,你 如何 能 识别 到 一个 从 任何地方 开始的 曲子。因为 多级临时记忆 区们 用 分布的 表示 ，该 区 用的 序列 记忆、推理 是 更 复杂的, 比 曲子 例子 蕴含的，但是 该例子 给出 一个 味道,它 怎么 工作。
- 它 可能 不 立即地(直白地) 明显， 但 每一个 你 曾经 有 的 感官的 体验,都是 新奇的(译者注:你的每一个感官体验都是不全等的)，然而 你 轻易地 发现 类似的 模式,在 新奇的 输入 (中)。例如，你 能 立即 词 "早饭",被 几乎 任何人 说出来的，不论 他们 是 老的 或 年轻的，男的 或 女的，说的 快 或 慢，或 有 很 重 的 口音。 即使 你 让 同一个 人 说  同样的 词 "早饭",一百 次，你的 耳蜗(音频 接收器) 从不  模拟到 两次,以 精确地 相等。
- 一个 多级临时记忆 区 面临 这个 相同的 问题,你 大脑 做的: 输入 可能 从不 重复,精确地。因此，恰 像 你的 大脑，一个 多级临时记忆  区 必须 处理 新奇的 输入,再 推理、训练 期间。一个 多级临时记忆 用 新 输入 复制,是 通过 稀疏 分布 表示们。一个 重要 特点,稀疏 分布  表示们, 是 你 只 需要 匹配 模式的  一 部分,(就) 有 把握 (确信) 匹配 是 显著的.

### - 预测

- 一个 多级临时记忆 的 每个 区 存储 模式们 的 序列们。 靠 匹配 存储的 序列们,和 当前输入， 一个 区 形成 一个 预测,关于 什么 输入 将 可能 下次 达到 。多级临时记忆 区们 实际上 存储了 稀疏 分布 表示们 间的 转变们。在 一些 实例 中,转变们 可以 看似 一个 线性 序列，比如 在 曲子 中的 音符们(译者注:音符是差量,非量自身)，但 在 一般 情况 (中),在  相同 时刻,多个 可能的 将来 输入 可能 被 预测(译者注:可以预测当前输入的下一个是单个、多个)。一个 多级临时记忆 区 会 做 不同的 预测,基于 上下文,在 时间 上 可能 向后 延伸 很远。序列 记忆、或存储 空间 模式 间 的 转变们 作用于  一个 多级临时记忆 的 主要 记忆  。 (译者注：从一个预测一个 叫 序列，从一个预测多个 叫 转变)
- 下面 是 多级临时记忆 预测 的 重要 属性。

#### - 预测 是 连续的。
- 没有 知觉 到 ：你 不停地 (在) 预测。多级临时记忆 做 同样的 (事)。当 听 一首 歌，你 在 预测 下一个 音符。当 下 楼梯，你 在 预测 何时 你的 脚 会 碰到 下一个 台阶。当 看 一个 棒球手 投递，你 在 预测 球 会 落 在 击球手 附近。在 一个 多级临时记忆 区 (中)，预测、推理 几乎 是 相同的 事情。预测 不是 一个 分离的 步骤,而是 分层例是记忆 区 工作方式 的 一部分。

#### - 预测 出现 在 该hierarchy 的 每个 层 的 每个 区。

- 如果 你 有 多级临时记忆 区们 的 一个hierarchy，预测 会 出现 在 每 层。区们 会 做 预测们 关于 他们 学到的 模式们。在 一个 语言 例子 中，低 级 区们 可能 预测 下一个 音素，而 高 级 区们 可能 预测 字们 或 短语们。

#### - 预测 是 上下文 敏感的。

- 预测 是 基于 过去 什么 已经 出现了，和 现在 什么 正 出现。(译者注:预测是基于过去的输入和现在的输入)。因此 一个 输入 会 产生 不同的 预测,基于 先前的 上下文。一个 多级临时记忆 区 学着 去 用,像 需要的 一样的 许多 前置 上下文，并且 能 保持 上下文,在 短、长 时间 展开 (上)。这个 能力 叫 可变 顺序 记忆。例如，考虑 一个 记忆的 演讲,例如 Gettysburg Address。要 预测 下一个 词，只 知道 当前 词 是 很难 够的；词 "and" 跟 在 "seven" 后,...

#### - 预测 导致 稳定

- 一个 区 的 输出 是 它的 预测。多级临时记忆 的 属性 之一 是,区们 的 输出 变得 更 稳定,它(区) 是 缓慢 改变的,在 该hierarchy 中, 他们(区们) 越高,持续 越久。这个 属性 从,一个 区 如何 预测,产生的。一个 区 不 只 预测,直接地 下一个 什么 将 发生。如果 它 能，它 将 预测 在 时间 上 提前 预测 多步。当 一个 新 输入 到达，该 新 预测 步 改变,但 先前的 四个 预测步 可能 不变。结果，即使 虽然 每个 新 输入 是 完全地 不同，只有 一部分 输出 正在 改变，使 输出们 比 输入们 更 稳定。这个 特性 镜像 我们 真实世界 的 经验，在 那,高 级 概念,比如 歌名字,改变 更慢 比 低 级 概念,这首歌 的 真实 音符。

#### - 一个 预测 告诉 我们,是否 一个 新 输入 是 期待的 或 非 期待的
- 每个 分层时刻记忆 区 是 一个 新奇 检测者。因为 每个 区 预测,接下来 将 出现 什么，它 "知道" 何时 未预料 的 事情 发生了。 分层时刻记忆们 能 预测 许多 可能的 下一个 输入们,同时地，不 仅仅 一个。所以 它 或许 不能 预测 精确地 下一个 将 发生 什么，但 如果 该 下一个 输入 不 匹配,任何 输入们,该 分层时刻记忆 区 会 知道 一个 异常 已经 出现了。

#### - 预测 帮助 使 该 系统 更 健壮,对 噪音
- 当 一个 分层时刻记忆 预测,下一个 什么 是 可能 发生的，该预测 能 让 系统 偏 向 推理,它 预测 过的。例如，如果 一个 分层时刻记忆 在 处理 说话 语言，它 会 预测  什么 声音们、词们、想法们 会 说出来,紧接着(下一个)。这个 预测 帮助 该 系统 填充 丢失 数据。如果 一个 歧义 声音 到达，该 分层时刻记忆 会 解释 该 声音,基于 它 期待 什么，因此 帮助 推理,即使 在 噪音 出现。
- 在 一个 分层时刻记忆 区 (中)，序列 记忆，推理，预测 是  亲密 集成的。他们 是 一个 区 的 核心 功能。

### - 行为
- 我们的 行为 影响,什么 我们 观察(到)。当 我们 移动 我们的 眼睛们，我们 的 视网膜 收到 正变化的 感官 输入。移动 我们的 肢体们、手指们 导致 变化的 触 感 到达 该 大脑。几乎 所有 我们的 动作 改变,什么 我们 感觉(到的)。感官 输入、运动神经的 行为 是 亲密地 纠缠着。
- (持续) 几十年(的) 主流 观点 是,在 该 新大脑皮层 (中),一个 单 区,主 运动神经 区,是 运动 命令 在 该 新大脑皮层 中 起源 的 地方。久而久之,发现,多数 或 全部区们,在 该 新大脑皮层 (中), 有 一个 运动 输出 ，即使 低 级 感官 区们。像是,全部 皮层 区们 集成 感官、运动 功能们。
- 我们期待,一个 运动 输出 能 被 加 到 每一个 分层时刻记忆 区,在 当前 存在的 框架 (内),因为 生成 运动 命令们 和 做 预测们 是 类似的。然后，全部 分层时刻记忆们 的 实现们,到 目前 为止,是 纯 感官的，没有 一个 运动 组件。

### - 过程,朝 多级临时记忆 实现
- 我们 已经 做 大量 过程,转化 该 分层时刻记忆 理论 框架 到 一个 实用 技术。我们 已经 实现 并 测试 了 若干给 版本 的 分层时刻记忆 皮层 学习 算法, 并 发现 知名的 基本 体系结构。当 我们 测试 该 算法 在 新 数据 集们 (上)，我们 会 改进 算法,添加 缺失的 块。我们 会 更新 这个 文档,当 我们 做 (的时候)。下 三 章 描述 该 算法们 的 当前 状态。
- 该 算法 有 许多 部件们,还 没有 实现，包括 注意力，区们 间 的 反馈，指定 时间，行为/感官-运动 集成。这些 缺失的 部件 适应 到 已经 创建的 框架 (中)。

## 2 多级临时记忆 皮层 学习 算法
- 这 一 章 描述 学习 算法们,一个 分层时刻记忆 区 内部 工作 的。第三章、第四章 描述 该 学习 算法们 的 实现,用 伪码，然而,这 一 章 是 更 概念化的。

### - 术语
- 在 我们 开始 之前，一个 笔记,关于 术语 可能 是 有帮助的。我们 用 神经科学的 语言,在 描述 分层时刻记忆 学习 算法 (上)。术语,例如 细胞们，synapse们，潜在 synapse们，dendrite 段们，列们 被 使用,自始至终。这个 术语 是 逻辑的,因为 该 学习 算法们 是 极大地 源自, 靠 匹配 神经科学 细节们,带着 理论 需要们。然后，在 实现 该 算法们 的 过程 中,我们 面临 性能 问题,因此 一旦 我们 感觉 我们 理解了 事情 怎么 工作的,我们 会 寻找 途径,去 加速 处理。 这 经常 引入 脱离,从 一个 严格 的 遵守 生物学 细节,直到 我们 能 获得 相同的 结果们。如果 你 是 神经科学 新手,这 不会 是 问题。然后，如果 你 熟悉 神经科学 术语，你 可能 发现 自己 困惑,因为 我们 我们 术语们 的 用法 变化,从 你的 期待。该 附录们,关于 生物学,讨论 差异们、类似们,在 该 分层时刻记忆 学习 算法们 和 他们的 神经生物学 对等物,在 细节上。 这里,我们 会 注意到 一些 分离,它 可能 导致 多数 困惑。

#### - 细胞 状态
- 分层时刻记忆 细胞们 有 三个 输出 状态，激活,从 前-喂 输入，激活,从 横向(译者注:兄弟) 输入(这 表示 一个 预测)，抑制。第一个 输出 状态 对应 一个 动作电位 的 短  脉冲,在 一个 神经元 (中)。第二个 输出 状态 对应 一个 更慢的，稳定 速率 的 动作电位,在 一个 神经元 (中)。我们 没 发现 一个 需要,对于 建模 对立 动作 电位们,或 甚至 在 这两个 激活 状态 上 的 速率 标量。分布 表示们 的 用法 看似,克服了  该 需要,去 建模 在 细胞们 中的 标量 激活。

#### - dendrite(译者注:非线性函数、激活函数) 段们
- 分层时刻记忆 细胞们 有 一个 相对 现实的 (因此 复杂) 的 dendrite 模型。理论上，每个 分层时刻记忆 细胞 有 一个 近  dendrit 段,一打或两个 远 dendrit 段们。 该 近 dendrite 段 接收 前-喂 输入,该 远 dendrite 段们 接收 横向(译注:兄弟) 输入,从 附近 细胞们。一类 抑制 细胞们 强迫 所有 该 细胞,在一个列 (中), 对应 到 类似的 前-喂 输入。为了 化简，我们 移除了 该 近 dendrite 段,从 每一个 细胞,并 替换 它 用 一个 单 共用 dendrite 段,每 列 的 细胞们。该 空间 共同 函数(下面 叙述) 操作 在 该 共用 dendrite 段，在 列们 级。该 时刻 共同 函数 操作 在 远 dendrite 段们，在 列 内 的 独立 细胞们 级 上。 这个 简化 达到 相同的 功能，虽然 在 生物学 (上),没有 对等物,跟 一个 dendrite 段,附着 到 一个 列。

#### - synapse(译者注:权重)们 
- 分层时刻记忆 synapse们 有 二值 权重。生物学 synapse 有 可变 权重,但 他们 也是 部分 随机的，意思是说 一个 生物 神经元 不 依赖 精确的 synaptic 权重们。 在 分层时刻记忆们 的 分布 表示们 的 使用,加上,我们 的 dendrite 操作 模型,允许 我们 赋予 二值 权重们  给 分层时刻记忆 synapse们,不 带有 病态 作用。为了 建模,synapse们 的 形成、没形成,我们 用 两个 额外的 概念,从 神经科学,你 可能 不熟悉的。一个 是 潜在 synapse们 的 概念。 **这 表示  所有 axon们,靠的 足够近 到 一个 dendrite 段,他们(axon和dendrite段) 能 潜在地 形成 一个 synapse。** 第二个 被 叫做 "紧度"。这 是 一个 标量 值,赋给 每一个 潜在 synapse。一个 synapse的 紧度 表示 一个axon 和 一个 dendrite 之间的 连接程度 的 范围。生物学地，这个范围 会 从 完全地 不连接的 synapse,到 开始 形成 一个 synapse 但 还 没 连接，到 一个 最小地 连接的 synapse，到 一个 极大地 完全的 连接的 synapse。一个 synapse 的 紧度 是 一个 标量值,变化 从 0.0 到 1.0。学习 包含 增加、减小 一个 synapse的 紧度。当 一个 synpase的 紧度 是 在 一个 阈值 之上，它 被 连接 以 权重 "1" 。 当 它 是 在 该 阈值 之下，它 断开 连接 以 权重 "0"。

### - 概要
- 想象 你 是 一个 分层时刻记忆 的  一个区。 你的 输入 包含 几千、几万 的 bits。这些 输入 bits 可能 表示 感官 数据,或 他们 可能 来自 另一个 在 该hierarchy中 更低的 区。他们 以 复杂的 方式  打开、关闭 。你 想 用 这个 输入 做 什么？
- 我们 已经 讨论了 答案,以 它的 最简单的 形式。每个 分层时刻记忆 区 寻找 在 它的 输入 中的 相同 模式们,然后 学习 这些 模式们 的 序列们。从 它的 序列们 的 记忆，每个 区 做 预测们。高 级 描述 使 它 听起来 容易，但 实际上 还有 许多 要 继续。让 我们 进一步 分解 它 到 下面 三步：
- 1 形成 该 输入的 一个 空间 分布 表示
- 2 在 先前 输入们 的 上下文 下,形成 该 输入的 一个 表示
- 3 在 先前 输入们 的 上下文 下,基于 当前 输入,形成 一个 预测
- 我们 会 讨论 每一步,在 更 细节 (上)。
- 1 形成 该 输入的 一个 空间 分布 表示
- 当 你 想象 到 一个 区 的 一个 输入 , 把 它 当成 一个 极大 数目的 bits。在 一个 大脑 (中),这些 是 来自 神经元们 的 axon们。 在 时间上 的 任何 一点, 这些 输入 bits 中的 一些 会 是 激活的(值1),其他的 会 是 失活的(值0)。输入 bits 中 激活 的 百分比 变化,据说 从 0% 到 60%。第一件 事情,一个 分层时刻记忆 区 做的,是 去 转化 这个 输入 到 一个 新 表示,稀疏的。例如，该输入中 可能 有  40%  bits  是 "开的",但 该 新 表示 只有   2%  bits 是 "开的"。
- 一个 分层时刻记忆 区  逻辑地 由 列 的 集合 组成。 每个 列 由 一个或多个 细胞 组成。列 可能 逻辑地 排列,以 二维 数组,但 这 不是 必须的。在 一个 区 中的 每一个 列,连到 输入 bits 的 一个 唯一 子集（经常 和 其他 列们 重叠,但 绝不 精确地 相同的 输入 bits 的 子集）。该 输入 的 该 空间 表示 被 编码,靠 哪个列 是 激活的,哪个列 在 抑制 后 是 失活的。抑制 函数 被 定义 为,达到 一个 相对 常量 百分比,激活的 列，即使 当 输入 bits 的 个数,激活的,显著地 变化。 
- 图 2.1: 一个 分层时刻记忆 区 由 细胞 列 组成。只 显示了 一个 区 的 一小部分 。每个 细胞 列 接收 激活,从 一个 唯一的 该输入 的 子集。**带有 最强 激活 的 列,　抑制，　带有 更弱 激活 的 列。结果 是 该输入的  一个 空间 分布 表示。** 该图 显示 激活 列,以 亮 灰色。（当 没有 前置 状态，在 激活 列 中 每个 细胞 会 被 激活，如同 显示的。）
- 想象,现在 该 输入 模式 改变了。如果 仅仅 少数 输入 bits 改变了，某些 列们 会 收到 多一点点 或 少一点点 输入 (bit),是 "开" 状态 ，但是 激活 列们 的 集合 不 大可能 变化 很多。因此 相似的 输入 模式（那些 有 显著 数量 的 共同 激活bits）会 映射 到 激活 列们 的 一个 相对地 稳定的  集合。编码的 稳定 程度 极大地 依赖于 每个 列 连到 什么 输入们 (上)。这些 连接 被 学到,通过 稍后 叙述 的 一个 方法。
- **所有 这些 步骤（学 到 这些 连接, 从 该 输入们 的 一个 子集 到 每个 列，决定  输入 到 每 列 的  级， 使用 抑制 去 选择 一个 激活 列们 的  稀疏 集合 ） 被 称作 "空间 (相似) 同 (表示)"(译注:空间 相似,则用同样的上层表示)。该 术语 意味着, "空间地" 相似的 模式 （意味着 他们 共享 大量 激活 bits ） 是(被) "合并的" （意味着 他们 以 同样的 表示 被 分 到 同一 组 ）**
- 2 在 先前 输入们 的 上下文 下,形成 该 输入的 一个 表示
- 下一个 功能,被 一个 区 执行的,是 去 转化 输入 的 圆柱状 表示 到 一个 新 表示,它 含有 状态,或 上下文,从 过去。**这个 新 表示 被 形成,靠 激活 在 每个 列  中的 该 细胞们 的 一个 子集， 典型地 每个 列 只 一个 细胞**(图 2.2)。
- 考虑 听到 两个 说出来的 句子，"I ate a pear" 和 "I have eight pears"。 字 "ate" 和 "eight" 是 同音字；他们 听起来 一样的。我们 能 确认 这个,在 某个 点,在 大脑 中,有 神经元们,他们 唯一地 对应 到 口语 "ate" 和 "eight"。毕竟，同样的 声音 进入了 耳朵。然后，我们 也 能 确定 这,在 另一个 点,在 大脑中,该 神经元们 对 这个 输入 的 响应 是 不同的,在 不同的 上下文。 发音 "ate" 的 表示 会  不同,当 你 听到 "I ate" 和 "I have eight"。 想象,你已经 记住 这 两个 句子 "I ate a pear" 和 "I have eight pears"。 听着 "I ate ..." 导致 一个 不同于 "I have eight ..." 的 预测。 必定 有 不同的 内部 表示,在 听着 "I ate" 和 "I have eight" 之后。
- 这个 原理,同一个 输入 (有) 不同的 编码,在 不同的 上下文,是 认知 和 动作 的  一个 通用 特征,　并且 (该原理) 是 一个 分层时刻记忆 去 的 最 重要的 功能 之一。过分强调 这个 能力 的 重要性 是 很难的。
- 在 分层时刻记忆 区 中的 每个 列 由  多个 细胞 组成。一个 列 中的 所有 细胞们 得到 相同的 喂-前 输入。在 一个 列 中的 每个 细胞 可以 被 激活 或 不 激活。 靠 选择 不同的 激活 细胞们 ,在 每个 激活 列 中，我们 能  表示 该 额外的 相同 输入 不同地,在 不同的 上下文。一个 具体的 例子 或许 由 帮助。说 没 个 列 由 4 个 细胞,每个 输入 的 表示 由 100 个 激活 列 表示。 如果 每 列 仅仅 一个 细胞,在 一个 时刻,我们 由 4^100 个 方式 表示 该 额外的 相同 输入。 **该相同 输入 会总是 导致 相同 的 100列 被 激活，但是 在 不同的 上下文,在 这些 列们 中的 不同的 细胞 会 被 激活。** 现在 我们 能 表示 相同的 输入,在 一个 非常 大量的 上下文 中，但是 怎么 区分 这些 不同的 表示 呢？ 近似地,全部 随即地  从 4^100种 可能模式 选择 对们,会 被 大约 25个 细胞 覆盖。因此 一个 特定 输入 的 两个 表示,在 不同 上下文 中,会 有 大约 25个 细胞 相同,75个 细胞 不同,使得 他们 容易地 被 区分。
- 被 一个 分层时刻记忆 区 使用的  通用规则 在 下面 (叙述)。当 一个 列 变成 激活的，它 着眼于 该列 中的 全部 细胞。如果 一个 或 多个 细胞,在 该 列 中,是 已经 在 预测 状态，仅仅 这些 细胞 边 激活。 如果 没有 细胞,在 该列 中,是 激活 状态，那么 全部这些细胞 变成 激活的。 **你 能 这样 考虑 它，如果 一个 输入 模式 是 被 期待的,那么 该 系统 确认 这个 期待,靠 激活 仅仅 在 预测 状态的 该 细胞们 。** 如果 该 输入 模式 是 意料之外的,那么 该 系统 激活 在 该列 中的 全部 细胞,仿佛 在说 "出现的 该 输入 是 意料之外的,故 全部 可能的 解释 是 合法的 "
- 如果 没有 前置 状态，因此 没有 上下文 和 预测，所有 在 一个 列 中 的 这些 细胞 将 变成 激活的,当 该列 变成 激活的。这个情节 是 类似的,和 听 一首歌 中的 第一个 音符。没有 上下文,你 通常 不能 预测,接下来 会 发生 什么；所有 选项 是 可用的。如果 有 前置 状态,但 输入 不 匹配 意料的，在 该 激活 列 中的 全部 细胞 会 变成 激活的。++这个 决定 被做,在 一列接一列 基础上,所以 一个 预测的 匹配 或 不匹配 绝不是 一个 "全部-或-空表" 事件。++
- 图2.2：靠 激活 细胞们 的 一个 子集,在 每 列 (中), 一个 分层时刻记忆 区 能 表示 相同 输入,在 多个 不同 上下文 (中)。列 仅仅 激活 预测的 细胞们。无 激活 细胞 的 列,激活 该列 中的 全部 细胞。该图 显示 某些 列,有 一个 细胞激活, 　某些 列,有 全部 细胞 激活。
- 像 提到的,在 上面 术语 段，分层时刻记忆 细胞 能 在 三个 状态 中的 一个。如果 一个 细胞 是 激活的,由于 喂-前 输入,我们 只 用 术语 "激活"。如果 该细胞 是 激活的 由于 兄弟(横向) 连接 到 其他 附近 细胞,我们称之为 "预测 状态"(图2.3)。
- 3 在 先前 输入们 的 上下问 下,基于 当前 输入,形成 一个 预测
- 对于 我们的 区 的 最终 步骤 是 区 做 一个 预测,接下来 什么 是 最可能 发生的。该预测 是 基于 在 步骤2 形成的 表示，它 包含 上下文,从 所有 先前的 输入们。 
- 当 一个 区 做 一个 预测,它(该区) 激活（进入 预测 状态）所有 该 细胞们,(这些细胞) 很可能 会 变成 激活,由于 将来的 喂-前 输入。因为 在 一个 区 (中的) 表示 是 稀疏的，在 相同 时刻,可以 做 多 预测。例如，如果 该 列们 的 2% 是 激活的,由于 一个 输入，你 可以 期待,做 十个 不同的 预测,(会) 导致 该 列们 的 20% 有 一个 预测的 细胞。或者，做 20个 不同的 预测,(会) 导致 该 列们 的 40% 有 一个 预测的 细胞。如果 每 列 有 四个 细胞，在 同一 时刻 有 1个(细胞) 激活的，那么 该 细胞们 的 10% 会 处于 预测 状态。
- 后面 一章,关于 稀疏 分布 表示,会 展示 尽管  不同的 预测 被 合并 在 一起，一个 区 能 高 概率地 知道,是否 一个 特定的 输入 是否 是 预测的  。
- 一个 区 怎么 做 一个 预测？当 输入 模式们 随 时间  改变，列们、细胞们  的 不同 集合们 按 序列 变 激活。当 一个 细胞 变成 激活的，它 形成 连接们,到 附近 细胞们 的 一个 子集,他们(附近细胞) 是 激活的 直接地 前置。这些 连接 能 被 快速地 或 慢速地 形成,(快慢) 依赖于 该 应用 需要的 学习 率。之后，一个 细胞 需要 做的 全部 是,看写 这些 连接,对 同时发生的 激活。如果 该 连接们 变 激活，该 细胞 可以 期待,它 可能 立刻 变 激活,并 进入 一个 预测 状态。因此 细胞 集合 的 该 喂-前 激活,典型地 会 导致 其他 细胞 集合 跟着 (进入) 预测的 激活。把 这个 当作 时刻,当 你 认出 一首 歌,并 开始 预测 下一个 音符们。
- 图 2.3：在  任意 时刻  点 上，在 一个 多层时刻记忆 区 中 的  一些 细胞 会 激活,由于 喂-前 输入（以 亮 灰色 显示）。其他 细胞,收到  兄弟 输入,从 激活的 细胞,会 处于 一个 预测的 状态（以 暗 灰色 显示）。 
- 总的来说，当 一个 新 输入 到达，它 导致 激活 列们 的 一个 稀疏 集合。在 每个 列 中 的 这些 细胞 中的 一个 或 多个 变成 激活的，这些 按序 导致 其他 细胞 进入 一个 预测 状态,通过 学到的 连接,在 该 区 的 细胞们 之间的。被 该 区 内 的 连接们 激活的 细胞们 组成 一个 预测,(关于) 接下来 什么 可能 发生。当 下一个 喂-前 输入 到达，它 选择了 另一个 激活的 列们 的 稀疏 集合 。如果 一个 新的 激活 列 是 非预期的，意味着 它 不是 预测的,被 任意 细胞，它 会 激活 在 这些 列 中的 全部 细胞。如果 一个 新的 激活 列 有 一个 或 多个 预测的 列们，只有 这些 细胞 会 变成 激活的。一个 区的 输出 是,在 该 区 中的 全部 细胞 的 激活，包含 由于 喂-前 输入 激活的 细胞们,在 预测的 状态 激活的 细胞们。
- 像 之前 提到的 (一样)，预测 不 只是 下一个 时间 步骤。在 一个 多层时刻记忆 区 中 的 预测们 能 有 几个 时间 步骤,在 将来。以 曲子 为 例子，一个 多层时刻记忆 区 不会 只 预测 下一个 音符,在 一个 曲子 中，而 可能 预测 下四个 音符。这 导致 一个 吸引人的 属性。一个 区 的 输出（在 一个 区 (中的),  全部 激活的、预测的 细胞们 的 并集）改变的 更慢 比 该 输入。想象 该区 正 预测 下四个 音符,在 一个 曲子 (中)。我们 可以 表示 该 曲子,按 字母 顺序 A,B,C,D,E,F,G。在 听到 首 两个 音符，该 区 识别到 该序列 并 开始 预测。它 预测 C,D,E,F。该 "B" 细胞们 已经 激活,所以 B,C,D,E,F 的 细胞 全部 在 该 两个 激活 状态 中的 一个。现在 该区 听到 下一个 音符 "C"。激活、预测 细胞们 的 集合 现在 表示 "C,D,E,F,G"。注意,该 输入 模式 完全地 改变,从 "B" 到 "C"，但 只 该 细胞们的 20% 改变了。
- 因为 一个 多层时刻记忆 区 的 输出 是一个 向量,表示 该区 的 所有 细胞们 的 激活，在 这个 例子 (中的) 输出 是 五倍 稳定,比 输入。在 一个 多级 区们 布局 (中)，当 你 (在) 该 多级 (中) 向上走,我们 会 看到 一个  临时(时刻) 稳定性 的 增加。
- 我们 用 术语 "时刻 pooler" 去 描述 该 两 步骤,增加 上下文 到 该 表示和预测。靠 创建 缓慢地 改变 模式们 的 序列们 的 输出，从 本质上,我们 "pooling" 不同的 模式 到 一起,(这些模式) 在 时刻上 跟随  彼此 。
- 现在 我们 会 进入 细节 的 另一 级。我们 开始 以 概念,被 空间相似则同表示 和 时刻相似则同表示 共享的。然后 我们 讨论 概念、细节,唯一 到 空间相似同表示,接着 是,概念、细节 唯一 到 时刻相似则同表示。

### - 共享的 概念

- 在 空间相似则同表示、时刻相似则同表示 上 学习 是 类似的。在 这 两种 情况 下 学习 包括 建立 连接们 或 synapse们,在 细胞们 之间。时刻相似则同表示 学习 喂-前 连接们,在 输入 bits 和 列们 之间。

#### - 二值 权重
- 多级时刻记忆 synapse们 有 只 0 或 1 起作用；他们的 "权重" 是 二值的，一个 属性,不像 许多 神经 网络 模型们,(他们) 用 标 量 值,在 范围 0 到 1。

#### - 性能
- 在 学习 期间, synapse们  不断地 形成,反形成 。像 之前 提到过的，我们 赋予 一个 标量 值 给 每一个 synapse （从0.0到1.0）,去 指示 多么 结紧,该 连接 是。当 一个 连接 被 加强，它的 结紧 被 提升。在 其他 条件们 下，该 结紧 被 降低。当 该 结紧 是 在 一个 阈值(比如 0.2) 之上，该 synapse 被 认为 是 建立了。 如果 该 结紧 低于 该 阈值，该 synapse 会 无效。

#### - dendrit 段们
- synapse们 连接 到 dendrit 段们。有 两 种 dendrit 段们，近 、 远。
- 一个 近 dendrit 段 形成 synapse们,用 喂-前 输入们。该 激活的 synapse们,在 这种 段,是 线性 加,以 决定了 一个列 的 该 喂-前 激活。
- 一个 远 dendrit 段 形成 synapse们,用 在 该 区 中的 细胞们 。每个 细胞 有 若干个 远 dendrite 段们。如果 该 激活 synapse们 的 累加和,在 一个 远 段,超出 一个 阈值, 然后 该 相关的 细胞 变成 激活,在 一个 预测 状态。因为,每个 细胞 有 多个 远 段，一个 细胞的 预测 状态 是,多个 阈值 检测 机构 的 逻辑或 操作。

#### - 潜在 synapse们
- 像 之前 提到过的，每个 dendrite段 有 一个 潜在 synapse 列表。所有 该 潜在 synapse们,被 给定 一个 结紧 值,可能 变成 起作用的 synapse们,如果 他们的 结紧 值 超出 一个 阈值。

#### - 学习
- 学习 包含 增加 或 减少 潜在 synapse们 的 结紧 值,在 一个 远 段 的。用于 使 synapse们 更多 或 更少 结紧 的 规则,(是) 类似 于 Hebbian 学习 规则。例如，如果 一个 后-synaptic 细胞 (是) 激活了,由于 一个 远 段,正接收 在 它 阈值 之上 的 输入，然后 在 该 段 上的  这些 synapse们 的  结紧 值 被 修改。 激活的 synapse们，因此 贡献 给 正 激活的 该 细胞，使 他们的 结紧 增加。非激活的 synapse们，因此 无 贡献，使 他们的 结紧值 降低。在 其他 条件 下, 结紧 值 更新 (方式) 不同,在 空间相似则同表示 和 时刻相似则同表示 (中)。下面 会 描述 细节。
- 现在 我们 要 讨论 空间相似则同表示、时刻相似则同表示 的 相关 内容。

### - 空间相似则同表示 概念
- 空间相似则同表示 的 最 基础的 函数,是 去 转化 一个 区的 输入 到 一个 空间 模式。该 函数 是 重要的,因为 原理,用于 学习 序列 、 做出 预测,以 空间 分布 表示 开始。
- 有 几个 覆盖的 目标,对于 空间相似则同表示，(他们) 决定老 空间相似则同表示 怎样 操作和学习。

#### - 1 用 全部 列
一个 多级时刻记忆 区  有 一个 固定 数目 的 列,(他们) 学习 去 表示 通用 模式,在 该 输入 (中的)。一个 目标,确保 全部 该 列们 学习 去 表示 有用的 某事物,(而) 忽略 你 有 多少 列。我们 不想 (要) 列们,从不 激活的。(为了) 阻止 这个 发生，我们 保持 跟踪,多么 频繁,一个 列 是 激活的,相对于 它的 邻居们。如果 一个 列的 该 相对 激活,是 太 低，直到 他 开始 成为 赢的 列 集合 的 一部分,他 (才会) 提高 他的 激活 水平。本质上来说，全部 列 (是) 在 竞赛,和 他们的 邻居们,以 变成 表示 输入 模式 的 一个 参与者。如果 一列 不是 很 激活，它 会 变的 更 进取。当 他 做了(激活了)，其他 列 将会 被迫 修改 他们的 输入,并 开始 稍微 不同地 表示 输入 模式。
#### - 2 维持 希望的(确定的) 密度
- 一个 区 需要 去 形成 一个 他的 输入们 的 稀疏 表示。带有 最多 输入 的 列们 抑制 他们的 邻居。抑制 半径,(它) 正比例于 该 列们的 感受野(易接受的区域) 的 尺寸（因此 能 从 小 变到 该 整个 区 的 尺寸 ）。在 抑制 半径 内，我们 允许 只 一 百分比的 列,带有 最多 激活 输入,以 变成 赢者。该 列们 的 剩余(列们) 被 禁用。（一个 抑制 半径 意味着 列们的 一个 二维 布局，但 该 概念 容易 适配到 其他 拓扑(布局)）.

#### - 3 避免 平凡 模式
- 我们 想让 全部 我们的 列 表示 在 输入 中的 非平凡 模式们。这个 目标 能 被 到达,靠 设置 输入的 一个 最小阈值,对于 要被 激活的 列 (的 输入)。例如，如果 我们 设置 该 阈值 为 50，它 意味着,一个 列 必须 有 一个 至少 50个 激活的 synapse,在 它的 要被 激活的 dendrit 段，(以) 保证 一个 确定的 复杂 水平, 它 表示的 该 模式 (的 复杂 水平)。

#### - 4 避免 额外 连接
- 如果  我们 不 小心，一个 列 能 形成 大量的 合法 synapse。然后,他 会 强烈地 响应,对 许多 不同的 不相关的 输入 模式。该 synapse们 的 不同 子集 对 不同的 模式  响应。为了 避免 这个 问题，我们 降低 任何 synapse的 结紧 值,(该synapse) 不是 当前地 贡献给 一个 赢的 列。靠 确保 非贡献 synapse们 是 不充分地 处罚，我们 保证 一个 列 表示 一个 受限 数目的 输入 模式，有时候  (该数目) 仅仅 (为) 一个。(译者注:没说充分的处罚是什么)

#### - 自 调整 感受野(易接受的 域)
- 真实的 大脑们 是 高度地 可塑的；该 新大脑皮层 的 区们 能  学习,去 表示 在 生理 上 完全地 不同的 事物  　 成　 各种各样的 差异(变化)。 如果 该 新大脑皮层的 (一)部分 被 损坏，其他 部分 会 调整 以 表示,损坏 部分 表示的 事物。如果 一个 感官 器官 被 损坏 或 被 改变，该 新大脑皮层 的 相关的 部分 会 调整 以 表示 其他 事物。该 系统 是 自-调整的。
- 我们 想让 我们的 多级临时记忆 区们 展现 同样的 灵活性。如果 我们 分配 10k 个 列 给 一个 区，它 应该 学会,如何 最佳 表示 该 输入,以 10k 个 列。如果 我们 分配 20k 个 列，它 应该 学会,如何 最佳 用 该 数目。如果 该 输入 统计 变了，该 列们 应该 改变 到 最佳 表示 该 新 实际。简言之，一个 多级时刻记忆 的 设计者 应该 能 分配 任意(多) 资源们 给 一个 区,并 该 区 会 做 它 能做的 最佳 工作,表示 该 输入,基于 该 可用的 列们 和 输入 统计。该 通用 规则 是,在 一个 区 中 带有 更多 列，每个 列 会 表示 在 　该 输入 中的 　更大、更多 详细 模式们。典型地,该 列们 也 会 更少 频繁的 激活，然而 我们 会 维持 一个 相对 不变的 稀疏 水平。
- 不 需要  新 学习的 规则 以 达到 这个 高度 渴望的 目标。靠 增加 非激活 列们，抑制 邻居 列们 以 维持 不变的 稀疏，建立 对 输入的 最小 阈值，维持 潜在 synapse们 的 一个 巨大 pool，并 基于 他们的 共享 添加、忘记 synapse们，全体 列 会 动态地 配置 以 达到 该 期望的 效果。

### - 空间相似则同表示 细节
- 我们 现在 可以 浏览  　 该 空间 pooling 函数 做的　  一切。
- 1 以  一个  　包含  固定 数量 bit 的　  输入  开始。这些 bit 可能 表示 感官 数据,或 他们 可能 来自 　在 该 多级 中 更低的　  另一个 区 。
- 2 赋予 一个 固定 数量 的 列,给 接收 该 输入 的 区。每个 列 有 一个 相关的 dendrit 端。每个 dendrit 段 有,一个 　 潜在 synapse 集合 表示 该输入 bit 的 一个 子集。每个 潜在 synapse 有 一个 紧结 值。基于 他们的 紧结 值，一些 潜在 synapse 会 变成 合法的。
- 3 对于 任意 给定 输入，决定 　在 每个 列 上　 有  多少个 合法的 synapse  连接 到 激活的 输入 bits。
- 4 激活的 synapse们 数目 被 乘以 一个 鼓励(boosting) 因子,(该因子) 动态地 由 　一个 列 相对于 它的 邻居们 的 激活频率　决定。
- 5 在 鼓励 后 　 带有 最高 激活 的 列们　 禁用  　 在 一个 抑制 半径 内的 　   全部,而 不是 一个 固定 百分比 　的  列们。该 抑制 半径 被 他 自己 动态地 决定,按 输入 bits 的 传播。现在 由 一个 激活 列 的 空间 集合 了。
- 6 对 该 激活 列们 的 每一个，我们 调整  　全部 该 潜在 synapse们的   　 紧结 值。跟 激活 输入 bits 对齐的　 synapse们的 紧结 值　被 增加了。 跟 非激活 输入 bits 对齐的　 synapse们的 紧结 值　被 降低了。 对 紧结 值们 的 修改 可能 改变 一些 synapse们,从 合法 到 不合法、或 相反。

### - 时刻相似则同表示 概念
- 和 上面 相似, 时刻相似则同表示 学习 序列,并 做 预测。基本 方法 是,当 一个 细胞 变成 激活的，它 形成 　 到 其他 细胞,曾 是 激活的,只 先前  　 的 连接。细胞们 然后 能 预测,当 他们 变成 激活的,靠 看 他们的 连接。如果 全部 该 细胞们 做 这个，他们 能 集体地 存储、回想 序列们，并且 他们 能 预测,接下来 可能 发生 什么。没有 中心化 存储,对 一个 模式们 的 序列；相反，记忆 是 分布在 个体 细胞们 之间。因为 该 记忆 是 分布的，该 系统 是 健壮的,对 噪音 和 错误。个体 细胞们 可以 失效，经常地 在 很难 或 不可辨别 的 作用 (下)。
- 它 是 值得的,注意到,temporal pooler 开发的,空间分布表示 的 一些 重要 属性 。
- 设想,我们 有 一个 假设的 区,它 总是 形成 表示们,靠 用 共 10k个 细胞 中的 200个 激活 细胞（在 任意 时刻 2% 的   细胞）。我们 怎么 能  记住、识别 　 200个 激活 细胞 的 　一个 特定 模式。做 这个 的 　一个 简单 方式  是  做 一个 列表,我们 在乎 的 这 200个 激活 细胞。如果 我们 看到 相同的 200个 细胞 再次 激活,我们 识别到了 该 模式。但是，如果 我们 做 一个 列表,只有 该 200个 激活 细胞 中的 20个,并 忽略 其他 180个 呢？会 发生 什么？你 可能 认为,记住 仅仅 20个 细胞,会 导致 许多 错误，这 20个 细胞 会 激活,以 200个 中 的 许多 不同 模式。但 它 不是 该 情形。因为 该 模式们 是 巨大的、稀疏的（你这个例子中,10k个细胞中200个激活细胞），记住 20个 激活 细胞 几乎 和 记住 全部200个 一样 好。在 实践 系统 中,错误 几率 极度地 小,同时 我们  相当大地 降低了 内存 需求。
- 在 一个 多级时刻记忆 区 中的 细胞们, 利用了 该 属性。每个 细胞的 dendrite 段们,有 一  连接 集合,(连接到) 该区中的 其他 细胞。一个 dendrite 段 形成 这些 连接,作为 含义:识别 该 网络 在 某 时间点 的 状态。在附近,可能 有 几百、几千 个 激活的 细胞,但 该 dendrite段 只 必须 连 到 其中 15或20个。当 该 dendrite段 看到 这些 激活 细胞 中的 15个，它 能 相当地 确定 较大的 模式 出现了。这个 技术 叫做 "子-采样",多级时刻记忆 算法 到处 使用了 (该技术)。
- 每个 细胞 参与,多个 不同 分布的 模式,多个 不同的 序列。一个 特定的 细胞 可能 是, 数打 或 数百 个 临时 转变, 的 一部分。因此 每个 细胞 有 若干个 dendrite 段, 不止 一个。理想地,一个 细胞 会 有 一个 dendrite段,给 每个 激活 模式,它 想 识别的。不过,实践地，一个 dendrite段 能 学习 　若干个 完全地 不同的 模式们 的 　连接们,并且 仍然 工作 良好。例如，一个 段 可能 学习 　每 4个 不同 模式 的　   20个 连接，对于 总共 80个 连接。然后 我们 设置 一个 阈值,以(使得) 该 dendrite段 变成 激活的,当  　 它的 连接 中的 　  任意 15个 是 激活的。这 引入了 错误 的 可能性。它 是 可能的，按 机会，该 dendrite 达到 它的 阈值 15 个 激活的 连接,靠 混合 不同 模式 的 一部分。然而，这种 错误 是 非常 不太 可能地，再次 由于 该 表示们 的 稀疏性。
- 现在 我们 能 看到,一个 细胞,带有 一打或两打 dendrite 段、数千个 synapse,能 识别 数百个  　细胞 激活 的 分离 状态。

### - 时刻相似则同表示 细节 
- 这里 我们 枚举 　被 时刻相似同表示　执行的 步骤们。我们 开始,不考虑 空间相似则同表示,带有 　表示 该 喂-前 输入　  的 激活 列 集合。
- 1 对 每个 激活的 列，检查　 在  预测 状态的 该 列　 中的 细胞，并 激活 他们。如果 没有 细胞 在 激活 状态，激活 　该 列 中的　 全部 细胞。激活 细胞 的 结果 集 是,在 前置 输入 上下文 中　的 该输入 的 表示。 
- 2 对于 　在 该 区 中的 每个 细胞 的  　每个 dendrite，计数  多少个 已建立的 synapse 是 已连接到 激活的 细胞们。如果 该 数目 超过 一个 阈值，该 dendrite 段 被 标记 为 激活的。细胞们,带有 激活的 dendrite 段们 的, 被 放在 预测 状态, 除非 他们 已经 　由于 喂-前 输入 (从而)　 是 激活的。细胞们,　带有:非 激活 dendrite们、由于 自下而上 的 输入 (从而) 非激活,　变成 或 保持 非激活。细胞 集合,现在 在 预测 状态的,是 该 区 的 预测。
- 3 当 一个 dendrite 段 变成 激活的，修改 　 与 该 段 相关的 　 所有 该  synapse们 的 紧结 值 。对于 每一个 潜在 synapse,在 激活的 dendrite 段 上的,增加 　这些 synapse们 　 的 该 紧结 值, (这些synpase们) 连 到 激活 细胞们;  降低  　这些 synapse们 　 的 该 紧结 值,(这些synpase们) 连 到 非激活 细胞们。对 synapse 紧结 值 的 这些改变, 被 标记 为 temporary(临时 或 时刻)。
- 这 修改了 在 段们 上的 该 synpase们 ,(这些synapse)已经 充分地 训练 以使得 该 段 激活，因此 导致 一个 预测。然而，我们 总是 想 扩展 预测们,更远 向后,在 时间 上,如果 可能。因此，我们    在 相同 细胞 上   挑了 一个 第二个 dendrite 段,去 训练。对于 该 第二个 段,我们 选择,那个 最 匹配    系统 在 上一 时间 步 的 状态   的。对于 这个 段，用  系统 在 上一 时间 步 的 状态，增加      连 到 激活 细胞们 的    那些 synapse们 的 紧结 值,降低      连 到 非激活 细胞们 的    那些 synapse们 的 紧结 值。对 synpase 的 紧结  的 这些 改变 被 标记 为 时刻(临时 temporary)。
- 4 每当    由于 喂-前 输入   一个 细胞 从 非激活 切换到 激活 ，我们 遍历 跟 该 细胞 关联的  每个 潜在的 synapse,并 移除 任何 temporary 标记。因此,只有 他们 正确地 预测了 该 细胞 的 该 喂-前 激活, 我们 (才) 更新 synapse们的 紧结。
- 5 当 一个 细胞 从 either 激活 切换到 非激活，   对   在 这个 细胞 上的     每个 潜在 synpase,撤销   被 标记 为 temporary 的     任何 紧结 改变。我们 不想 加强     不正确地 预测了 一个 细胞的 喂-前 激活 的       synapse们 的 紧结。

- 注意,仅仅     由于 喂-前 输入 而 激活      的 细胞    (能) 在 该 区 内    传播 激活 ，其他 预测们 会 导致 进一步的 预测。但 该 所有 激活 细胞（喂-前、预测的）形成  一个 区 的 输出,并 传播 到 在 多级 中的 下一个 区 。

### - 第一 序 versus 变 序 序列、预测
- 在 我们 结束 关于 空间相似则同表示、时刻相似则同表示 的 我们的  讨论 之前,有 一个 更 主要的 话题 要 讨论。可能 不是 所有 读者 都 对 它 感兴趣,并且 (对于) 理解 第三章、第四章,它 也是 非必需的。
- ...

## 3 空间 池化 实现、伪码
- 这一章 包含     空间相似则同表示 功能 的     第一个实现 的      详细的 伪码 。给 这段 代码 的   输入,是     来自 感官 数据 或 先前 级 的      一个    自下而上的 二进制 输入     的  数组 。该 代码 计算 activeColumns(t) -- 在 时刻 t, 由于 该 自下而上 的 输入, 而 赢 的 列 列表。这个 列表     作为输入     被 发送 到     下一章 描述的   时刻相似则同表示 例程，即 activeColumns(t) 是 该 空间相似则同表示 例程 的 输出。
- 该 伪码 被 分割 成      按顺序 出现的      三个 不同的 阶段:
- 阶段1 : 对 每个 列,用 当前 输入,计算 覆盖
- 阶段2 : 在 抑制 后,计算 赢的 列们
- 阶段3 : 更新 synapse 紧结 和 内部变量
- 虽然 空间相似则同表示 学习 是 天生 在线地 (学习)，(但) 简单地 取消 阶段3,你 (就) 可以 关掉 学习。
- 本章 剩余(部分) 包含 三阶段 中 每一个 的 伪码。在 末尾,定义了        在 该 代码 中 用到的  各种 数据结构 和 支持的例程 。

### 初始化
- 在 接收 任何 输入 之前，靠    对 每个 列    计算 初始 潜在 synapse 列表,以 初始化 该区。这 包含    从 该 输入 空间     选择的  输入们 的随机集合。每个 输入 用    一个 synapse,并 给(该synapse) 赋以 随机 紧结 值       来 表示。该 随机 紧结 值  用 两个标准 选出来的。  第一，该 紧结指们 是 从       在 connectedPerm(为中心)的小区间       中 选择的。connectedPerm(译注:允许连接:允许成为已连接的界限紧结值). connectedPerm(一个synpase被认为是"已连接"的最小紧结值)。在 少量的 训练 伦次 后, 这 (就) 能使得(启用) 潜在synapse们 变成 已连接（或已断开）。第二，每个 列  以    该输入区的自然中心      为 中心(译注:不太确定这句话意思,是列的中心还是区的中心)，朝 这个 中心,紧结值们  有一个 偏执（在中心附近,紧结有更高的值）。

### 阶段1:覆盖
- 给定 一个 输入 向量，用 该 向量, 阶段1 计算 每个列的 覆盖。每个列的 覆盖 简单(来说) 是   跟 激活 输入 连接的 synapse们的 个数        乘以      它的 鼓励(boost)。如果 这个值 (是) 低于 minOverlap，我们    把 该overlap 设置 为零。
```python
1. for c in columns
2. 
3.     overlap(c) = 0
4.     for s in connectedSynapses(c)
5.         overlap(c) = overlap(c) + input(t,s.sourceInput)
6. 
7.     if overlap(c) < minOverlap then
8.         overlap(c) = 0
9.     else
10.        overlap(c) = overlap(c) * boost(c)
```

### 阶段2:抑制
- 第二 阶段 
- 第二阶段 计算,在 抑制 步骤 后,哪些列 作为 赢者 剩余。desiredLocalActivity 是 一个  　 控制 最后 赢 的 列 个数 的　    参数。例如，如果 desiredLocalActivity 是 10，一个 列 会 是 赢者,如果 他的 overlap 分 比 　在 抑制半径 内 　的 第10高 的 列 的 分数 高。
```pypthon
11. for c in columns
12. 
13.     minLocalActivity = kthScore(neighbors(c),desiredLocalActivity)
14. 
15.     if overlap(c) > 0 and overlap(c) >= miniLocalActivity then
16.         activiColumns(t).append(c)
17.
```

### 阶段3:学习
- 第三阶段 执行 学习，他 更新 全部 必要的 synapse们的 紧结值，也 更新 鼓励 和 抑制半径。
- 主要 学习 规则 实现在 行 20-26。对于 赢的 列们，如果 一个 synapse 是 激活的 ，他的 紧结值 是 增加的，否则 他的 紧结值 是 降低的。紧结值 被 限制在 0 到 1 之间。
- 行 28-36 实现 鼓励。在 合适的 位置,有 两个 分离的 鼓励 方法,帮助 一个 列 学习 连接们。如果 一个列 不是 赢的 足够 频繁（以activeDutyCycle测量）。他的 全部 鼓励值 会 降低（行30-32）。Alternatively，如果 一个列的 已连接的 synapse们 没有 和 任何输入 覆盖的 足够频繁（以overlapDutCycle测量），他的 紧结值 会 被 鼓励（行34-36）。注意：一旦 学习 被 关闭，c的鼓励 被 冻结。
- 最后，阶段三的结尾,重算 抑制半径（行38）。

```python
18. for c in activeColumns(t):
19. 
20.     for s in potentialSynapses(c):
21.         if active(s) then
22.             s.permannence += permannenceInc
23.             s.permannence = min(1.0,s.permannence)
24.         else
25.             s.permannence -= permannenceDec
26.             s.permannence = max(0.0,s.permannence)
27. 
28. for c in columns:
29. 
30.     minDutyCycle(c) = 0.01 * maxDutyCycle(neighbors(c))
31.     activeDutyCycle(c) = updateActiveDutyCycle(c)
32.     boost(c) = boostFunction(activeDutyCycle(c),minDutyCycle(c))
33.     
34.     overlapDutyCycle(c) = updateOverlapDutyCycle(c)
35.     if overlapDutyCycle(c) < minDutyCycle(c) then
36.         increasePermanences(c,0.1*connectedPerm)
37. 
38. inhibitionRadius = averageReceptiveFieldSize()
```


### 支持的 数据结构 和 例程
- 在 伪码 中, 用到了 下面的 变量 和 数据结构。

| 名字 | 解释 |
| :----------:| :----- |
| columns | 列表,全部列 |
| input(t,j) | 在时刻t,给 该级 的 输入。如果第j个输入是开,则input(t,j)为1 |
| overlap(c) | 在一个特定输入模式下,列c的 空间相似则同表示 的 overlap |
| activeColumns(t) | 由于自下而上的输入,赢的列下标列表 |
| desiredLocalActivity | 在抑制步骤后,控制赢的列个数,的参数 |
| inhibitionRadius | 列们的平均连接接收区域尺寸 |
| neighbors(c) | 在列c的inbitionRadius内的全部列 |
| minOverlap | 在抑制步骤期间,一个不被淘汰的列,必须有的最少激活输入 |
| boost(c) | 列c的鼓励值 |
| synapse | 一个 数据结构 表示 一个 synapse----包含 一个 紧结值 和 源输入下表  |
| connectedPerm | 如果 一个 synapse的 紧结值 比connectedPerm大,就说 它 是 已连接的。 |
| potentialSynapses(c) | c的潜在synapse列表,以及(这些)synapse的紧结值 |
| connectedSynapses(c) | potentialSynapses(c)的一个子集,在该子集中,紧结值 比 connectedPerm大。有 自下而上的输入,当前 连接到 列c。 |
| permanenceInc | 在 学习 期间,synapse们的 紧结值 被 增加的量 |
| permanenceDec | 在 学习 期间,synapse们的 紧结值 被 降低的量 |
| activeDutyCycle(c) | (c的激活负载周期). 一个 滑动 平均值,表示, 在 c 抑制 后,c被 激活的 频率  |
| overlapDutyCycle(c) | (c的overlap负载周期).一个 滑动 平均值,表示  在它的输入(下) 列c 有 显著 overlap（比如 比minOverlap大）的频率。（比如 最近 1k个 轮次） |
| minDutyCycle(c) | (c的最小占空比;c的最小负载周期).对一个细胞,表示 最小希望开火速率 的一个变量。如果 一个 细胞的 开火速率 低于 该值，它 会 被 鼓励。该值 被 计算 为,它的邻居们的最大开火速率的1%。 |

- 下面的 支持 例程 被 用 在 以上 代码 (中)。

| 名字 | 解释 |
| :----------:| :----- |
| kthScore(cols,k) | 给定列列表cols,返回 第k高的(译:还是前k个) overlap值 |
| updateActiveDutyCycle(c) | 计算 一个 移动 平均值,在 抑制 后,c 被 激活 的 频率  |
| updateOverlapDutyCycle(c) | 计算 一个 移动 平均值,在 c 有          比miniOverlap 大的  overlap     的频率 |
| averageReceptiveFieldSize(c) | 全部 列 的 已连接的接收域 尺寸 的 平均半径。一个列 的 已连接的接收域 尺寸 只 包含 该 已连接的 synapse们（这些synapse的permanence值>=connectedPerm）。这 被 用于 决定    在 列 之间 的 兄弟(横向) 抑制 的 范围。  |
| maxDutyCycle(c) | 在 一个 给定 列 列表 (中),返回,该 列们的 最大 激活负载周期。   |
| increasePermanences(c,s) | 按 一个 标量 因子 s,增加 在 列 c 中的 每一个 synapse的 紧结值。  |
| boostFunction(c) | 返回 一个 列的  鼓励值。该 鼓励值 是 一个 >=1 的 标量。如果 activeDutyCycle(c) 高于 miniDutyCycle(c), 该 鼓励值 是1。 一旦 该 列 的 activeDutyCycle 开始 下落 到 他的 minDutyCycle 之下, 该 鼓励值 线性地 增长。  |



## 4 临时相似则同表示 实现、伪码
- 这章包含了  临时相似则同表示 功能 的第一个实现  的 详细伪码。给该代码的输入是activeColumns(t),已被 空间相似则同表示 计算过了。在 当前时刻t，对每个细胞,该代码 计算  激活和预测 状态。对 每个 细胞 的 激活状态、预测状态 的 逻辑OR操作  形成了 临时相似则同表示的  给 下一级 的 输出。
- 伪码 被 分离 成 按序列出现的 三个 独立的 阶段：
- 阶段1：对 每个细胞，计算 激活状态，activeState(t)。
- 阶段2：对 每个细胞，计算 预测状态，predictiveState(t)。
- 阶段3：更新synapses
- 只有在学习的时候,才需要阶段3。然而，不像 空间相似则同表示，当启用学习,阶段1和阶段2 包含 一些 学习-特定的 操作。既然 临时相似则同表示 是 比 空间相似则同表示  显著的 更复杂，我们 首先 列出 只-推理 版本的 临时相似则同表示，紧接着 一个版本,组合 推理和学习。一些 该实现细节、术语、支持例程 的 描述 是 在 本章结尾，在伪码之后。

### - 临时相似则同表示 伪码：只 推理

#### - 阶段1

- 对 每个 细胞,第一解阶段 计算 激活状态。对每个 赢的列,我们 决定 哪些 细胞 应该 变成 激活的。如果 如何一个细胞 预测 自下而上的 输入（即,由于 在先前一个时间步 的 一个 序列 段, 他的 predictiveState是1），然后 这些 细胞 变成 激活的（行4-9）。如果 该 自下而上的 输入 是 非预期的（即,没有 细胞 有 predictiveState输出 开），那么 在 该列中的 每个 细胞 变成 激活的（行11-13）。

```python
01. for c in activeColumns(t):
02. 
03.     buPredicted = false
04.     for i = 0 to cellPerColumn - 1
05.         if predictiveState(c,i,t-1) == true then
06.             s = getActiveSegment(c,i,t-1,activeState)
07.             if s.sequenceSegment == true then
08.                 buPredicted = true
09.                 activeState(c,i,t) = 1
10.                 
11.     if buPredicted == false then
12.         for i = 0 to cellPerColumn -1 
13.             activeState(c,i,t) = 1
```
#### - 阶段2
- 对 每个 细胞,第二个阶段 计算 预测状态。如果 它的segments的任何一个变成激活的,一个细胞会变打开它的predictiveState，即,由于 喂-前 输入,如果 足够个 它的horizontal连接们 当前开火。

```python
14. for c,i in cells
15.     for s in segments(c,i)
16.         if segmentActive(c,i,s,t) then
17.             predictiveState(c,i,t) = 1
```
### - 临时相似则同表示 伪码：联合 推理 和 学习

#### - 阶段1
- 第一阶段计算 在一个赢的列中的 每个细胞的 activeState。对于 这些 列，该代码 进一步的,对 每个列 选择 一个细胞,作为学习的细胞（learnState）。逻辑如下：如果 任何一个细胞 预测了 自下而上的输入（即,由于一个序列段,它的predictiveState输出是1 ），然后这些细胞变成激活的（行23-27）。如果该段变成激活的,从用 learnState开 选出的细胞，该细胞被选作学习细胞（行28-30）。如果该自下而上的输入是非预期的，最佳匹配细胞被选chosen as学习细胞（行36-41）,并且 一个新段被加到该细胞中。

```python
18. for c in activeColumns(t)
19. 
20.     buPredicted = false
21.     lcChosen = false
22.     for i = 0 to cellPerColumn - 1
23.         if predictiveState(c,i,t-1) == true then
24.             s = getActiveSegment(c,i,t-1,activeState)
25.             if s.sequenceSegment == true then
26.                 buPredicted = true
27.                 activeState(c,i,t) =1 
28.                 if segmentActive(s,t-1,learnState) then
29.                     lcChosen = true
30.                     learnState(c,i,t) = 1
31.                     
32.     if buPredicted == false then
33.         for i = 0 to cellPerColumn -1
34.             activeState(c,i,t) =1 
35.             
36.     if lcChosen == false then
37.         l,s = getBestMatchingCell(c,t-1)
38.         learnState(c,i,t) = 1
39.         sUpdate = getSegmentActiveSynapses(c,i,s,t-1,true)
40.         sUpdate.sequenceSegment = true
41.         segmentUpdateList.add(sUpdate)
```

#### - 阶段2
对每个细胞,第二阶段计算预测状态。如果 该细胞的段变成激活的,该细胞会打开它的预测状态输出，即,如果 足够的 它的横向输入 当前是 激活的,由于 喂-前 输入。在这个情况下，该细胞 排队 下列 变化：a) 强化,当前激活段（行47-48），并且 b) 强化 一个段,该段 已预测了此激活，即,一个段,该段 有一个（潜在的 弱的） 匹配 到 激活,在先前时间步（行50-53）。
```python
42. for c,i in cells
43.     for s in segments(c,i)
44.         if segmentActive(s,t,activeState) then
45.             predictiveState(c,i,t) = 1
46.             
47.             activeUpdate = getSegmentActiveSynapses(c,i,s,t,false)
48.             segmentUpdateList.add(activeUpdate)
49.             
50.             predSegment = getBestMatchingSegment(c,i,t-1)
51.             predUpdate = getSegmentActiveSynapses(c,i,predSegment,t-1,true)
52.             
53.             segmentUpdateList.add(predUpdate)
```

#### - 阶段3
- 第三个、最后一个阶段实际上在实施学习。在这个阶段,已被排队的段更新 实际被执行,一旦 我们 获得 喂-前 输入,且 该细胞 被选作一个学习的细胞（行56-57）。否则，如果该细胞 曾经 由于任何原因 而 停止预测，我们 否定地 强化 这些段（行58-60）。
```python
54. for c,i in cells
55.     if learnState(s,i,t) == 1 then
56.         adaptSegments(segmentUpdateList(c,i),true)
57.         segmentUpdateList(c,i).delete()
58.     else if predictiveState(c,i,t) == 0 and predictiveState(c,i,t-1) == 1 then
59.         adaptSegments(segmentUpdateList(c,i),false)
60.         segmentUpdateList(c,i).delete()
61.         
```

### - 实现细节和术语
- 在这一段,我们描述我们的 临时相似则同表示 的 实现和术语的 一些细节。每个细胞用两个数索引：一个列索引c，一个细胞索引i。细胞们维护一各dendrite段列表，在那,每个段包含  一个synapse列表 加 每个synapse的一个紧结值。对一个细胞的synapses的修改 被标记为 临时的,直到该细胞变成激活的从喂-前输入。这些临时的改变被维护在segmentUpdateList。每个段也维护一个boolean标志sequenceSegment，指示 在下一个时间步,是否 该段 预测 喂-前 输入。
- 潜在synapses的实现 是 不同于 空间相似则同表示 中的。在空间相似则同表示中，潜在synapses的完整列表被表示为一个明确的列表。在临时相似则表示中，每个段有它自己的（可能很大）潜在synapses列表。在实践中,为每个段,维护一个长列表 是 计算昂贵的和内存密集的。因此在临时相似则同表示中，我们随机地添加激活的synapses到每个段,在学习期间（被参数newSynapseCount控制）。这个优化有一个类似的效果和维护潜在synapses完整列表，但是每个segment的列表是远远更小的,当仍然维护学习的新临时模式的可能性。
- 伪码也用一个小状态机去追踪细胞状态在不同时间步。我们维护三个不同状态,对每个细胞。数组activeState和predictiveState追踪在每个时间步的每个细胞的激活和预测状态。数组learnState决定哪个细胞的输入被适用,在学习期间。当一个输入时非预期的，在一个特定列中的所有该细胞变成激活的,在该相同的时间步。只有这些细胞之一（该细胞最匹配输入）使它的learnState打开。我们只加  从 让learnState设置为1的cells的 synapses（这避免老 过表示 一个全的激活列,在dendritic segments）。
- 以下数据结构被用在临时相似则同表示的伪码中：

| 名字 | 解释 |
| :----------:| :----- |
| cell(c,i) | 一个细胞列表，用i和c索引 | 
| cellsPerColumn | 在每个列中的细胞个数 | 
| activeColumns(t) | 由于自下而上的输入 而 赢的 列 下标 列表（这是空间相似则同表示的输出） | 
| activeState(c,i,t) | 一个boolean向量,每个细胞一个数字。它表示,在时刻t,给定当前喂-前输入,过去临时上下文,列c细胞i的激活状态。activeState(c,i,t)是 从列c细胞i,在时刻t 的贡献。如果为1，该细胞有当前喂-前输入,也有一个近似的临时上下文。 | 
| predictiveState(c,i,t) | 一个boolean向量,每个细胞一个数字。它表示,在时刻t,给定其他列的自下而上的激活,过去临时上下文,列c细胞i的预测。predictiveState(c,i,t)是 从列c细胞i,在时刻t 的贡献。如果为1，该细胞预测喂-前输入,在当前临时上下文。 | 
| learnState(c,i,t) | 一个boolean,指示 是否 列c细胞i 被选作学习细胞 | 
| activationThreshold | 一个段的激活阈值。如果 在一个段中的 激活已连接的synapse个数 比 activationThreshold 大，称 该段 被激活了 | 
| learningRadius | 在一个临时相似则同表示细胞周边的区域,从该区域,该细胞能获得横向连接 | 
| initialPerm | 对一个synapse的初始紧结值 | 
| connectedPerm | 如果一个synapse的紧结值比此值大，称作已连接 | 
| minThreshold | 学习的最小段激活 | 
| newSynapseCount | 在学习期间,添加到一个段的synapses的最大个数 | 
| permanenceInc | 当基于激活的学习出现,synapses的紧结值的增量 | 
| permanenceDec | 当基于激活的学习出现,synapses的紧结值的减量 | 
| segmentUpdate | 持有 需要去更新一个给定段 的三块信息：a)段索引（如果他是一个新段,则为-1），b) 存在的激活的synapses的一个列表，c) 一个标志,指示是否此段应该被标记为一个序列段（默认为false）  | 
| segmentUpdateList | segmentUpdate结构列表。 segmentUpdateList(c,i)是 列c细胞i的变更列表 | 

- 下面的支持例程被用在以上代码中：

| 名字 | 解释 |
| :----------:| :----- |
| segmentActive(s,t,state) | 由于时刻t的state(导致)该段是激活的, 如果  在段s上的已连接的synapse个数 大于activationThreshold,则此例程返回true。参数state可以是activeState,或者learnState | 
| getActiveSegment(c,i,t,state) | 对给定列c细胞i，返回一个 段的segmentActive(s,t,state)为true的 段下标。如果多段是激活的，序列segments是被偏爱的。否则，偏爱 带有最多激活的 段。 | 
| getBestMatchingSegment(c,i,t) | 在时刻t,对给定列c细胞i，找到 带有最大激活synapses数目 的 段。在找到最匹配,此例程是侵略性的。synapses的紧结值允许低于connectedPerm。激活的synapses个数允许低于activatonThreshold，但是必须高于minThreshold。此例程返回该段下标。如果没找到(任何)段，则返回下标-1。 | 
| getBestMatchingCell(c) | 对于给定列c，返回 带有最匹配段（如上定义）的 细胞。如果没有细胞有一个匹配段，则返回 带有最少段个数的 细胞。 | 
| getSegmentActiveSynapses(c,i,t,s,newSynapses=false) | 对段s,返回 包含一个计划的修改们 的 一个segmentUpdate数据结构。在时间步t,让activeSynapses成为激活的synapses列表,(在该列表中)起源细胞有他们的activeState输出为1.。（如果 因该段不存在 而 s为-1,则该列表为空。）newSynapses是一个可选参数,默认为false。如果newSynapse是true，那么newSynapseCount-count(activeSynapses)个synapses被加到activeSynapses中。这些synapses是 在时间步t,有learnState输出为1 的细胞集合 中 随机地选出的。
| adapatSegments(segmentList,positiveReinforcement) | 该函数遍历一个segmentUpdate列表,并强化每一个段。对每一个segmentUpdate元素，一下修改被执行。如果positiveReinforcement是true,那么在激活列表中的synapses的紧结值按permanenceInc增加。所有其他synapses的紧结值按permanenceDec降低。如果positiveReinforcement是false，那么在激活列表中的synapses紧结值按permanenceDec降低。在这步骤之后，在segmentUpdate中的任何synapses如果已经做过了(刚刚的操作),则用initialPerm加到紧结值。 | 

## 附录 A：生物 神经元 、多级临时记忆 细胞 比较
### - 生物神经元
### - 简单 人工 神经元
### - 多级临时记忆 细胞
### - 建议 阅读



## 附录 B: 新大脑皮层、多级临时记忆 区 比较
### - 新大脑皮层 电路
### - 为什么 有 层 和 列？
### - 假设,关于 不同 层们 做 什么
### - 总结

## 术语
